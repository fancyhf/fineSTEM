from fastapi import APIRouter, HTTPException, Body
from pydantic import BaseModel
import os
import httpx
from typing import List, Optional

router = APIRouter(prefix="/chat", tags=["Chat"])

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    context: Optional[str] = None
    provider: Optional[str] = None # å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨ env ä¸­çš„ AI_PROVIDER

def get_provider_config(requested_provider: Optional[str] = None):
    """
    è·å–æŒ‡å®šæˆ–é»˜è®¤ AI æä¾›å•†çš„é…ç½®ã€‚
    """
    # 1. ç¡®å®šæä¾›å•†ï¼šè¯·æ±‚å‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤(deepseek)
    provider = requested_provider
    if not provider:
        provider = os.getenv("AI_PROVIDER", "deepseek")
    
    provider = provider.lower()
    
    config = {
        "api_key": None,
        "base_url": None,
        "model": None,
        "name": provider
    }

    if provider == "siliconflow":
        config["api_key"] = os.getenv("SILICONFLOW_API_KEY")
        config["base_url"] = os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1")
        config["model"] = os.getenv("SILICONFLOW_MODEL", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
    elif provider == "deepseek":
        config["api_key"] = os.getenv("DEEPSEEK_API_KEY")
        config["base_url"] = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        config["model"] = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
    elif provider == "openai":
        config["api_key"] = os.getenv("OPENAI_API_KEY")
        config["base_url"] = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        config["model"] = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        
    return config

@router.post("/completions")
async def chat_completions(request: ChatRequest):
    """
    ä½¿ç”¨é…ç½®çš„ AI æä¾›å•†ï¼ˆSiliconFlow, DeepSeek ç­‰ï¼‰å¤„ç†èŠå¤©è¡¥å…¨ã€‚
    å¦‚æœæœªé…ç½® API å¯†é’¥ï¼Œåˆ™è¿”å›ç”¨äºæ¼”ç¤ºçš„æ¨¡æ‹Ÿå“åº”ã€‚
    """
    # æ ¹æ®æä¾›å•†åŠ è½½é…ç½®
    config = get_provider_config(request.provider)
    
    api_key = config.get("api_key")
    base_url = config.get("base_url")
    model = config.get("model")
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰çœŸå®çš„å¯†é’¥ï¼Œæˆ–è€…æ˜¯å¦åº”è¯¥ä½¿ç”¨æ¨¡æ‹Ÿå“åº”
    if not api_key or api_key == "sk-placeholder" or api_key.startswith("sk-placeholder"):
        # print(f"ç”±äºç¼ºå°‘ {config['name']} çš„å¯†é’¥ï¼Œæ­£åœ¨ä½¿ç”¨æ¨¡æ‹Ÿå“åº”")
        return simulate_response(request.messages, request.context)

    # 2. å‡†å¤‡åŒ…å«ä¸Šä¸‹æ–‡çš„ç³»ç»Ÿæç¤ºè¯
    system_prompt = "ä½ æ˜¯ä¸€ä½ä¹äºåŠ©äººçš„å°‘å„¿ç¼–ç¨‹ AI å¯¼å¸ˆã€‚è¯·ç”¨ç®€å•æ¸…æ™°çš„è¯­è¨€è§£é‡Šä»£ç ã€‚"
    if request.context:
        system_prompt += f"\n\nå½“å‰ä»£ç /ä¸Šä¸‹æ–‡ï¼š\n{request.context}"

    full_messages = [{"role": "system", "content": system_prompt}] + [m.dict() for m in request.messages]

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": full_messages,
                    "stream": False
                },
                timeout=30.0
            )
            
            if response.status_code != 200:
                print(f"AI æä¾›å•† ({config['name']}) é”™è¯¯: {response.text}")
                return simulate_response(request.messages, request.context)
                
            data = response.json()
            return {"role": "assistant", "content": data["choices"][0]["message"]["content"]}
            
    except Exception as e:
        print(f"èŠå¤©æ¥å£é”™è¯¯: {e}")
        return simulate_response(request.messages, request.context)

def simulate_response(messages: List[Message], context: Optional[str]):
    """
    Fallback simulation when no API key is present.
    """
    last_user_msg = messages[-1].content.lower()
    
    if "åŒæ‘†" in last_user_msg or "double pendulum" in last_user_msg:
        return {
            "role": "assistant",
            "content": "åŒæ‘†æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç‰©ç†ç³»ç»Ÿï¼å®ƒçš„è¿åŠ¨æ˜¯'æ··æ²Œ'çš„ï¼Œæ„æ€æ˜¯è¯´ï¼Œå“ªæ€•ä½ åªæ˜¯æ”¹å˜ä¸€ç‚¹ç‚¹åˆå§‹ä½ç½®ï¼Œæœ€åçš„æ ·å­éƒ½ä¼šå®Œå…¨ä¸ä¸€æ ·ã€‚è¿™å°±åƒæ˜¯'è´è¶æ•ˆåº”'å“¦ï¼ğŸ¦‹"
        }
    elif "é‡åŠ›" in last_user_msg or "gravity" in last_user_msg:
        return {
            "role": "assistant",
            "content": "é‡åŠ›å°±åƒæ˜¯åœ°çƒçš„ä¸€åªå¤§æ‰‹ï¼Œä¸€ç›´æŠŠæ‰€æœ‰ä¸œè¥¿å¾€ä¸‹æ‹‰ã€‚åœ¨æˆ‘ä»¬çš„ä»£ç é‡Œï¼Œ`engine.world.gravity.y` å°±æ§åˆ¶ç€è¿™ä¸ªåŠ›é‡çš„å¤§å°ã€‚å¦‚æœä½ æŠŠå®ƒè®¾ä¸º 0ï¼Œå°çƒå°±ä¼šé£˜èµ·æ¥ï¼Œåƒåœ¨å¤ªç©ºä¸­ä¸€æ ·ï¼ğŸš€"
        }
    elif "python" in last_user_msg:
        return {
            "role": "assistant",
            "content": "Python æ˜¯ä¸€ç§éå¸¸æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œç‰¹åˆ«é€‚åˆåšæ•°æ®åˆ†æå’Œäººå·¥æ™ºèƒ½ã€‚åœ¨ Track E é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° Python åœ¨ 2012 å¹´ä¹‹åçªç„¶å˜å¾—è¶…çº§å—æ¬¢è¿ï¼Œè¿™éƒ½å¤šäºäº† AI çš„å‘å±•å‘¢ï¼ğŸ"
        }
    else:
        return {
            "role": "assistant",
            "content": "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼ä½œä¸ºä½ çš„ AI ç¼–ç¨‹åŠ©æ‰‹ï¼Œæˆ‘å¯ä»¥å¸®ä½ è§£é‡Šè¿™æ®µä»£ç æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ä½ å¯ä»¥è¯•ç€é—®æˆ‘å…³äº'é‡åŠ›'ã€'æ‘©æ“¦åŠ›'æˆ–è€…'æ’åºç®—æ³•'çš„é—®é¢˜å“¦ï¼(æ³¨æ„ï¼šå½“å‰æœªé…ç½® DeepSeek API Keyï¼Œä»…ä¸ºæ¨¡æ‹Ÿå›å¤)"
        }
