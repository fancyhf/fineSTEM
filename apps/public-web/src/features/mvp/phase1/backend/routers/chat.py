from fastapi import APIRouter, HTTPException, Body
from pydantic import BaseModel
import os
import httpx
from typing import List, Optional

router = APIRouter(prefix="/chat", tags=["Chat"])

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]
    context: Optional[str] = None
    provider: Optional[str] = None # å¦‚æœä¸º Noneï¼Œåˆ™ä½¿ç”¨ env ä¸­çš„ AI_PROVIDER

def get_provider_config(requested_provider: Optional[str] = None):
    """
    è·å–æŒ‡å®šæˆ–é»˜è®¤ AI æœåŠ¡æä¾›å•†çš„é…ç½®ã€‚
    """
    # 1. ç¡®å®šæä¾›å•†: è¯·æ±‚å‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤(deepseek)
    provider = requested_provider
    if not provider:
        provider = os.getenv("AI_PROVIDER", "deepseek")
    
    provider = provider.lower()
    
    config = {
        "api_key": None,
        "base_url": None,
        "model": None,
        "name": provider
    }

    if provider == "siliconflow":
        config["api_key"] = os.getenv("SILICONFLOW_API_KEY")
        config["base_url"] = os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1")
        config["model"] = os.getenv("SILICONFLOW_MODEL", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
    elif provider == "deepseek":
        config["api_key"] = os.getenv("DEEPSEEK_API_KEY")
        config["base_url"] = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        config["model"] = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
    elif provider == "openai":
        config["api_key"] = os.getenv("OPENAI_API_KEY")
        config["base_url"] = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        config["model"] = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
    
    return config

@router.post("/completions")
async def chat_completions(request: ChatRequest):
    """
    å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚ï¼Œæ”¯æŒåŠ¨æ€åˆ‡æ¢ AI æä¾›å•†ã€‚
    """
    config = get_provider_config(request.provider)
    
    # ç³»ç»Ÿæç¤ºè¯å·¥ç¨‹
    system_prompt = "You are a helpful STEM education assistant."
    if request.context:
        system_prompt += f"\nContext: {request.context}"
    
    # æ„é€ å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
    full_messages = [{"role": "system", "content": system_prompt}] + [m.model_dump() for m in request.messages]

    # æ£€æŸ¥æ˜¯å¦æœ‰ API Keyï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
    if not config["api_key"]:
        print(f"[è­¦å‘Š] æœªæ‰¾åˆ°æä¾›å•† {config['name']} çš„ API Keyã€‚ä½¿ç”¨æ¨¡æ‹Ÿå“åº”ã€‚")
        return {
            "role": "assistant",
            "content": f"[æ¨¡æ‹Ÿå“åº” ({config['name']})] è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„ AI å›å¤ã€‚è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® {config['name'].upper()}_API_KEY ä»¥è·å¾—çœŸå®å›å¤ã€‚"
        }

    headers = {
        "Authorization": f"Bearer {config['api_key']}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": config["model"],
        "messages": full_messages,
        "temperature": 0.7
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{config['base_url']}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60.0
            )
            
            if response.status_code != 200:
                print(f"AI æœåŠ¡æä¾›å•† ({config['name']}) é”™è¯¯: {response.text}")
                raise HTTPException(status_code=response.status_code, detail=f"AI æä¾›å•†é”™è¯¯: {response.text}")
                
            data = response.json()
            # é€‚é… OpenAI æ ¼å¼çš„å“åº”
            return data["choices"][0]["message"]
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"èŠå¤©è¡¥å…¨å¼‚å¸¸: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def simulate_response(messages: List[Message], context: Optional[str]):
    """
    Fallback simulation when no API key is present.
    """
    last_user_msg = messages[-1].content.lower()
    
    if "åŒæ‘†" in last_user_msg or "double pendulum" in last_user_msg:
        return {
            "role": "assistant",
            "content": "åŒæ‘†æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„ç‰©ç†ç³»ç»Ÿï¼å®ƒçš„è¿åŠ¨æ˜¯'æ··æ²Œ'çš„ï¼Œæ„æ€æ˜¯è¯´ï¼Œå“ªæ€•ä½ åªæ˜¯æ”¹å˜ä¸€ç‚¹ç‚¹åˆå§‹ä½ç½®ï¼Œæœ€åçš„æ ·å­éƒ½ä¼šå®Œå…¨ä¸ä¸€æ ·ã€‚è¿™å°±åƒæ˜¯'è´è¶æ•ˆåº”'å“¦ï¼ğŸ¦‹"
        }
    elif "é‡åŠ›" in last_user_msg or "gravity" in last_user_msg:
        return {
            "role": "assistant",
            "content": "é‡åŠ›å°±åƒæ˜¯åœ°çƒçš„ä¸€åªå¤§æ‰‹ï¼Œä¸€ç›´æŠŠæ‰€æœ‰ä¸œè¥¿å¾€ä¸‹æ‹‰ã€‚åœ¨æˆ‘ä»¬çš„ä»£ç é‡Œï¼Œ`engine.world.gravity.y` å°±æ§åˆ¶ç€è¿™ä¸ªåŠ›é‡çš„å¤§å°ã€‚å¦‚æœä½ æŠŠå®ƒè®¾ä¸º 0ï¼Œå°çƒå°±ä¼šé£˜èµ·æ¥ï¼Œåƒåœ¨å¤ªç©ºä¸­ä¸€æ ·ï¼ğŸš€"
        }
    elif "python" in last_user_msg:
        return {
            "role": "assistant",
            "content": "Python æ˜¯ä¸€ç§éå¸¸æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œç‰¹åˆ«é€‚åˆåšæ•°æ®åˆ†æå’Œäººå·¥æ™ºèƒ½ã€‚åœ¨ Track E é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° Python åœ¨ 2012 å¹´ä¹‹åçªç„¶å˜å¾—è¶…çº§å—æ¬¢è¿ï¼Œè¿™éƒ½å¤šäºäº† AI çš„å‘å±•å‘¢ï¼ğŸ"
        }
    else:
        return {
            "role": "assistant",
            "content": "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼ä½œä¸ºä½ çš„ AI ç¼–ç¨‹åŠ©æ‰‹ï¼Œæˆ‘å¯ä»¥å¸®ä½ è§£é‡Šè¿™æ®µä»£ç æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ä½ å¯ä»¥è¯•ç€é—®æˆ‘å…³äº'é‡åŠ›'ã€'æ‘©æ“¦åŠ›'æˆ–è€…'æ’åºç®—æ³•'çš„é—®é¢˜å“¦ï¼(æ³¨æ„ï¼šå½“å‰æœªé…ç½® DeepSeek API Keyï¼Œä»…ä¸ºæ¨¡æ‹Ÿå›å¤)"
        }
